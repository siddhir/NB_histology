{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ba6497-1e68-41d0-9883-3c18c41ee1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['SF_BACKEND'] = 'torch'\n",
    "os.environ['SF_SLIDE_BACKEND'] = 'libvips'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import slideflow as sf\n",
    "from slideflow.slide import qc\n",
    "import pandas as pd\n",
    "import slideflow.clam\n",
    "import fastai\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, auc, f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dd52ff-457b-4b66-8f5b-ea0cb08809c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.about()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72f99d4-6da8-433a-b0dc-ff98643bb8c3",
   "metadata": {},
   "source": [
    "# Tile extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc3d2cb-1b11-4f81-86ba-72562d6b40d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P = sf.Project('/home/sramesh/PROJECTS/NB')\n",
    "# P.annotations = \"./annotations_updated_031824.csv\"\n",
    "# dataset = P.dataset(tile_px=299, tile_um=302, filters={'site': ['3'], 'QC':['Pass']}) \n",
    "\n",
    "# qc_list = [qc.GaussianV2(), qc.Otsu()]\n",
    "\n",
    "# dataset.extract_tiles(qc=qc_list, grayspace_fraction=1, roi_method='inside', img_format='jpg',skip_extracted=False, dry_run=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daf9876-8d0d-405b-8df6-56c2c742ba56",
   "metadata": {},
   "source": [
    "# Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25f0b16-7378-4659-b0ba-1a7eefed5849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from slideflow.model import build_feature_extractor\n",
    "\n",
    "# # Build extractor\n",
    "# ctranspath = sf.model.build_feature_extractor('ctranspath', tile_px=299)\n",
    "# features_pt=P.generate_features(ctranspath,dataset=dataset, normalizer = 'reinhard_mask')\n",
    "\n",
    "# # Export feature bags.\n",
    "# features_pt.to_torch('/home/sramesh/PROJECTS/NB/pt_files/NB_ctranspath')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba8ff24-ba54-4d77-ba10-85264683876d",
   "metadata": {},
   "source": [
    "# Dataset Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bcfe6c-16dd-4d39-9d1f-27a299f460ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = sf.Project('/home/sramesh/PROJECTS/NB')\n",
    "\n",
    "P.annotations = '/home/sramesh/PROJECTS/NB/annotations_updated_031824.csv'\n",
    "full_dataset = P.dataset(\n",
    "    tile_px=299, \n",
    "    tile_um=302, \n",
    "    filters={'QC': ['Pass']},\n",
    "    min_tiles=20,\n",
    "    sources=[\"NB-1\", \"NB-2\", \"NB-3\", \"NB-4\",\"NB-external\"]\n",
    ")\n",
    "outcome_dataset = full_dataset.filter({'Diagnosis':['Neuroblastoma'],\n",
    "#'Grade':['Poorly Differentiated', 'Differentiating']})\n",
    "#'MKI':['Low/intermediate','High']})\n",
    "'MYCN':['Non-Amplified','Amplified']})\n",
    "\n",
    "print(\"All slides/tfrecords:\", len(full_dataset.tfrecords()))\n",
    "print(\"Outcome-specific slides/tfrecords:\", len(outcome_dataset.tfrecords()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17968b5e-4a26-425d-818a-34def9bf3832",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = outcome_dataset.annotations['MYCN'].value_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad40de59-42b5-4e0b-88e7-aaadc748a7fb",
   "metadata": {},
   "source": [
    "### Slide Manifest Reconciliation:\n",
    "</br>188 training slides\n",
    "</br>184 extracted (47, 97-3640, 4 - corrupted; 99-1955 - no tiles passed qc + path review said exclude)\n",
    "</br>176 - 8 additional slides failed QC\n",
    "</br>172 - 4 slides had less than 20 tiles extracted\n",
    "</br>\n",
    "</br>25 - external cohort\n",
    "</br>\n",
    "</br>Total - 197 (Note: This is only if all outcome labels are available, if subsets of patients have missing labels then total count is reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f706aab3-90d7-4717-b2fd-dfef18be5391",
   "metadata": {},
   "source": [
    "### Splits for k-fold and internal/external validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc66eb81-bde7-46f3-bdc2-8bdc799097d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset \n",
    "# train,val = outcome_dataset.split(\n",
    "#     val_fraction=0.15,\n",
    "#     labels='Grade',\n",
    "#     splits='/home/sramesh/PROJECTS/NB/splits/split_Grade_8515-2.json'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ef6c33-dd4f-4afe-9280-dd4b8d0f0e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits=outcome_dataset.kfold_split(\n",
    "#     k=5,\n",
    "#     labels='Grade',\n",
    "#     splits='/home/sramesh/PROJECTS/NB/splits/split_Grade_5fold.json'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a36989-67b3-439e-b1fd-7ec0e954a6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset for train/test\n",
    "train = outcome_dataset.filter({'site':['1','2']})\n",
    "\n",
    "val = outcome_dataset.filter({'site':['3']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdab84a7-7fbb-49fe-b462-28494787d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train size: '+str(len(train.tfrecords()))+', '+ 'Val size: '+ str(len(val.tfrecords())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55cd9c0-e828-47fd-9cb5-10aeb1dba947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from slideflow.mil import mil_config\n",
    "\n",
    "config = mil_config(\n",
    "    'attention_mil', \n",
    "    wd= 1e-05,\n",
    "    bag_size= 256,\n",
    "    #fit_one_cycle= False,\n",
    "    epochs= 10,\n",
    "    batch_size= 32,\n",
    "    #lr=7e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c208ff8-ebb4-410e-b5d1-db67a8ea610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on each cross-fold\n",
    "# fold_counter=1\n",
    "# for train, val in splits:\n",
    "#     P.train_mil(\n",
    "#         config=config,\n",
    "#         outcomes='Grade',\n",
    "#         train_dataset=train,\n",
    "#         val_dataset=val,\n",
    "#         bags='/home/sramesh/PROJECTS/NB/pt_files/NB_ctranspath',\n",
    "#         exp_label=f'NB_ctranspath_RHMnorm_Grade-NB-kfold-{fold_counter}',\n",
    "#         attention_heatmaps=True\n",
    "#     )\n",
    "#     fold_counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5657cee3-3b87-4510-a0a3-cc429f0a67a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MIL with fixed internal-external split\n",
    "P.train_mil(\n",
    "    config=config,\n",
    "    train_dataset=train,\n",
    "    val_dataset=None,\n",
    "    outcomes='MYCN',\n",
    "    bags='/home/sramesh/PROJECTS/NB/pt_files/NB_ctranspath',\n",
    "    exp_label='NB_ctranspath_RHMnorm_MYCN-fulltrain',\n",
    "    attention_heatmaps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1c9ade-cee6-4f7b-aadb-5b64c6e6724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate a saved MIL model\n",
    "\n",
    "P.evaluate_mil(\n",
    "    '/home/sramesh/PROJECTS/NB/mil/00099-NB_ctranspath_RHMnorm_MYCN-fulltrain-revised/',\n",
    "    outcomes='MYCN',\n",
    "    dataset=val,\n",
    "    bags='/home/sramesh/PROJECTS/NB/pt_files/NB_ctranspath',\n",
    "    attention_heatmaps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e1493-0a64-46b3-ab79-7daba1c0df32",
   "metadata": {},
   "source": [
    "# K-Fold Results Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5f0ba7-925b-494b-9cdb-29704a140590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For k-fold\n",
    "\n",
    "# Initialize an empty DataFrame to store aggregated data\n",
    "# aggregated_data = pd.DataFrame()\n",
    "# k=1\n",
    "\n",
    "# # Loop through each fold and aggregate data\n",
    "# for i in range(89, 94):\n",
    "#     file_name = f'/home/sramesh/PROJECTS/NB/mil/000{i}-NB_ctranspath_RHMnorm_Grade-NB-kfold-{k}/predictions.parquet'\n",
    "#     data = pd.read_parquet(file_name)\n",
    "#     aggregated_data = pd.concat([aggregated_data, data])\n",
    "#     k+=1\n",
    "\n",
    "# For external validation\n",
    "file_name = f'/home/sramesh/PROJECTS/NB/mil_eval/00038-attention_mil-MKI_fulltrain-revised/predictions.parquet'\n",
    "aggregated_data = pd.read_parquet(file_name)\n",
    "\n",
    "# Define a threshold for classification\n",
    "threshold = 0.5\n",
    "\n",
    "# Classify as 1 if y_pred1 > threshold, else 0\n",
    "aggregated_data['prediction'] = np.where(aggregated_data['y_pred1'] > threshold, 1, 0)\n",
    "\n",
    "# Calculate TP, FN, TN, FP\n",
    "TP = np.sum((aggregated_data['y_true'] == 1) & (aggregated_data['prediction'] == 1))\n",
    "FN = np.sum((aggregated_data['y_true'] == 1) & (aggregated_data['prediction'] == 0))\n",
    "TN = np.sum((aggregated_data['y_true'] == 0) & (aggregated_data['prediction'] == 0))\n",
    "FP = np.sum((aggregated_data['y_true'] == 0) & (aggregated_data['prediction'] == 1))\n",
    "\n",
    "# Calculate Sensitivity, Specificity, Precision, Recall, F1, AUPRC, AUROC\n",
    "sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "recall = sensitivity  # Recall is equivalent to sensitivity\n",
    "f1 = f1_score(aggregated_data['y_true'], aggregated_data['prediction'])\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(aggregated_data['y_true'], aggregated_data['y_pred1'])\n",
    "auprc = auc(recall_curve, precision_curve)\n",
    "auroc = roc_auc_score(aggregated_data['y_true'], aggregated_data['y_pred1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a15130-26c1-42eb-b86f-8df7a4a66ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the metrics\n",
    "print(\"MKI Aggregated Metrics:\")\n",
    "print(f\"Average Sensitivity: {sensitivity:.4f}\")\n",
    "print(f\"Average Specificity: {specificity:.4f}\")\n",
    "print(f\"Average Precision: {precision:.4f}\")\n",
    "print(f\"Average Recall: {recall:.4f}\")\n",
    "print(f\"Average F1-Score: {f1:.4f}\")\n",
    "print(f\"Average AUPRC: {auprc:.4f}\")\n",
    "print(f\"Average AUROC: {auroc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28bf20a-a200-4a98-b87b-ec64bddf8e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask for correct predictions\n",
    "is_correct_prediction = (aggregated_data['y_pred1'] > threshold) == aggregated_data['y_true']\n",
    "\n",
    "# Filter the DataFrame for correct predictions\n",
    "correct_predictions_df = aggregated_data[is_correct_prediction]\n",
    "\n",
    "# Display the DataFrame with correct predictions\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(correct_predictions_df)\n",
    "\n",
    "correct_predictions_df.to_csv('MKI-correct-predictions-external.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f6b49-ba4a-4e3a-b0ed-51e8af0bffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the index of the DataFrame to ensure no duplicate indices\n",
    "correct_predictions_df_reset = correct_predictions_df.reset_index(drop=True)\n",
    "\n",
    "# Plotting the distribution again\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=correct_predictions_df_reset, x=\"y_pred1\", hue=\"y_true\", bins=20, kde=True)\n",
    "plt.title(\"Distribution of Correct Predictions by Confidence\")\n",
    "plt.xlabel(\"Prediction Confidence\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"True Label\", labels=[\"1\", \"0\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82199a99-8243-477f-b4c7-b1fc3f254495",
   "metadata": {},
   "source": [
    "# Sensitivity vs. Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f458be28-3e62-4948-bfff-36add9400b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet('/home/sramesh/PROJECTS/NB/mil_eval/00037-attention_mil-MYCN_fulltrain/predictions.parquet')\n",
    "\n",
    "# Define a threshold for classification\n",
    "threshold = 0.5\n",
    "\n",
    "# Classify as 1 if y_pred1 > threshold, else 0\n",
    "data['prediction'] = np.where(data['y_pred1'] > threshold, 1, 0)\n",
    "\n",
    "# Calculate True Positives, False Positives, True Negatives, and False Negatives\n",
    "TP = np.sum((data['y_true'] == 1) & (data['prediction'] == 1))\n",
    "FN = np.sum((data['y_true'] == 1) & (data['prediction'] == 0))\n",
    "TN = np.sum((data['y_true'] == 0) & (data['prediction'] == 0))\n",
    "FP = np.sum((data['y_true'] == 0) & (data['prediction'] == 1))\n",
    "\n",
    "# Calculate Sensitivity and Specificity\n",
    "sensitivity = TP / (TP + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "print(f\"Sensitivity: {sensitivity}\")\n",
    "print(f\"Specificity: {specificity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cab7d49-107a-4af0-94c4-c82ce3917127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Precision and Recall\n",
    "precision, recall, _ = precision_recall_curve(data['y_true'], data['y_pred1'])\n",
    "auprc = auc(recall, precision)\n",
    "\n",
    "# Calculate Precision (Positive Predictive Value)\n",
    "precision_value = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1 = f1_score(data['y_true'], data['prediction'])\n",
    "\n",
    "print(f\"Precision: {precision_value}\")\n",
    "print(f\"AUPRC: {auprc}\")\n",
    "print(f\"F1-score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72968c28-9fb0-482a-bf8a-05fe8ce17a02",
   "metadata": {},
   "source": [
    "# Incorrect Prediction Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7350d8f3-73dc-473c-a28b-c411503228d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify as 1 if y_pred1 > threshold, else 0\n",
    "data['prediction'] = np.where(data['y_pred1'] > threshold, 1, 0)\n",
    "\n",
    "# Filter out the rows where the prediction is incorrect\n",
    "data_wrong = data[data['y_true'] != data['prediction']]\n",
    "\n",
    "# Output the rows where the prediction is incorrect\n",
    "print(data_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40f717f-dd06-4856-9772-e65018a949af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify as 1 if y_pred1 > threshold, else 0\n",
    "data['prediction'] = np.where(data['y_pred1'] > threshold, 1, 0)\n",
    "\n",
    "# Filter out the rows where the prediction is incorrect\n",
    "data_correct = data[data['y_true'] == data['prediction']]\n",
    "\n",
    "# Output the rows where the prediction is incorrect\n",
    "print(data_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d6bfad-0114-4e1d-bd98-c822319f6acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from slideflow.mil import predict_slide\n",
    "from slideflow.slide import qc\n",
    "\n",
    "# Load a slide and apply Otsu thresholding\n",
    "slide = '/home/sramesh/labshare/SLIDES/UCH_APPLEBAUM/May7_Scans/Mark_Applebaum-042_05-07.svs'\n",
    "wsi = sf.WSI(slide, tile_px=299, tile_um=302)\n",
    "wsi.qc(qc.Otsu())\n",
    "\n",
    "# Calculate predictions and attention heatmap\n",
    "model = '/home/sramesh/PROJECTS/NB/mil/00013-NB_ctranspath_RHMnorm_dx'\n",
    "y_pred, y_att = predict_slide(model, wsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaec96e-705a-40c8-bdba-65f1682d90ae",
   "metadata": {},
   "source": [
    "# UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b749b1d-9b60-4669-920d-01676ac6243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctranspath = sf.model.build_feature_extractor('ctranspath', tile_px=299)\n",
    "\n",
    "# # Generate DatasetFeatures from an extractor\n",
    "# ftrs = sf.DatasetFeatures(ctranspath, full_dataset, normalizer= 'reinhard_mask',cache='/home/sramesh/PROJECTS/NB/UMAP/ctranspath.pkl')\n",
    "\n",
    "# # Create the base UMAP\n",
    "# slide_map = ftrs.map_activations()\n",
    "\n",
    "# # Load annotations (site, outcomes)\n",
    "# outcome_labels, _ = full_dataset.labels('Diagnosis_binary_old', format='name')\n",
    "\n",
    "# # Label UMAP by outcome\n",
    "# slide_map.label_by_slide(outcome_labels)\n",
    "# #slide_map.save_plot('/home/sramesh/PROJECTS/NB/UMAP/ctranspath_Diagnosis_binary_old.png', s=5)\n",
    "# ftrs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msca",
   "language": "python",
   "name": "msca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
